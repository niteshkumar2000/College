{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Plagarism Checker - 17PW24"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add dataset from the problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "data = [ \r\n",
    "    'Information requirement: query considers the user feedback as information requirement to search',\r\n",
    "    'Information retrieval: query depends on the model of information retrieval used',\r\n",
    "    'Prediction problem: Many problems in information retrieval can be viewed as prediction problems',\r\n",
    "    'Search: A search engine is one of applications of information retrieval models',\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split title and content from the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "titles = [] \r\n",
    "contents = []\r\n",
    "for doc in data:\r\n",
    "    split_data = doc.strip().split(':')\r\n",
    "    titles.append(split_data[0].rstrip().lower())\r\n",
    "    contents.append(split_data[1].lstrip())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get all stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "stopwords_all = stopwords.words(\"english\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize and lemmatize the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# import nltk\r\n",
    "# nltk.download('wordnet')\r\n",
    "\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\r\n",
    "\r\n",
    "englishLemmatizer = WordNetLemmatizer()\r\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\r\n",
    "\r\n",
    "# For title\r\n",
    "title_set = set([])\r\n",
    "for title in titles:\r\n",
    "    lematized_word_list = [englishLemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(title) if word not in stopwords_all]\r\n",
    "    title_set.update(lematized_word_list)\r\n",
    "print(title_set)\r\n",
    "\r\n",
    "# For content\r\n",
    "content_set = set([])\r\n",
    "for content in contents:\r\n",
    "    lematized_word_list = [englishLemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(content) if word not in stopwords_all]\r\n",
    "    content_set.update(lematized_word_list)\r\n",
    "print(content_set)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'requirement', 'prediction', 'information', 'search', 'retrieval', 'problem'}\n",
      "{'user', 'search', 'query', 'information', 'requirement', 'prediction', 'retrieval', 'viewed', 'many', 'used', 'application', 'problem', 'feedback', 'depends', 'one', 'model', 'considers', 'a', 'engine'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up a pipeline of several functions as prescribed in the problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "def get_preprocessed_content(contents):\r\n",
    "    content_set = set([])\r\n",
    "    for content in contents:\r\n",
    "        lematized_word_list = [englishLemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(content) if word not in stopwords_all]\r\n",
    "        content_set.update(lematized_word_list)\r\n",
    "    return list(content_set)\r\n",
    "\r\n",
    "def fit_count_frequency(vocabulary, contents, binary=False):\r\n",
    "    term_freq = CountVectorizer(vocabulary=vocabulary, binary=binary).transform(contents).toarray()\r\n",
    "    return term_freq\r\n",
    "\r\n",
    "def get_tfidf(vocabulary, contents):\r\n",
    "    term_freq = fit_count_frequency(vocabulary, contents, binary=False)\r\n",
    "    weights_term_per_doc = TfidfTransformer().fit_transform(term_freq).toarray()\r\n",
    "    return term_freq, weights_term_per_doc\r\n",
    "\r\n",
    "def fit_tfidf(vocabulary, contents):\r\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),('tfid', TfidfTransformer())]).fit(contents)\r\n",
    "    term_freq = pipe['count'].transform(contents).toarray()\r\n",
    "    weights_term_per_doc = pipe['tfid'].fit_transform(term_freq).toarray()\r\n",
    "    return term_freq, weights_term_per_doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing the values from the defined functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "title_array = fit_count_frequency(list(title_set), titles, True)\r\n",
    "print(title_array)\r\n",
    "\r\n",
    "term_freq, weights_term_per_doc = fit_tfidf(list(content_set), contents)\r\n",
    "print(weights_term_per_doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0]\n",
      " [0 1 0 0 0 1]\n",
      " [0 0 0 1 0 0]]\n",
      "[[0.42580171 0.33570696 0.33570696 0.222201   0.42580171 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.42580171 0.         0.         0.         0.42580171 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.38014737 0.25161565 0.         0.\n",
      "  0.30776206 0.         0.         0.48216873 0.         0.\n",
      "  0.         0.48216873 0.         0.48216873 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.2720387  0.         0.52130524\n",
      "  0.33274238 0.52130524 0.52130524 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.43391936 0.         0.28720678 0.         0.\n",
      "  0.35129512 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.55037169 0.         0.         0.\n",
      "  0.55037169]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define function to compute cosine similarity\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def cosine_similarity_fn(td_matrix, query_w):\r\n",
    "    numerator = np.matmul(td_matrix,np.transpose(query_w)).reshape(1,-1)\r\n",
    "    square_sum_td = np.sum(np.square(td_matrix),axis=1)\r\n",
    "    square_sum_query = np.repeat(np.sum(np.square(query_w)),td_matrix.shape[0])\r\n",
    "    denom = np.multiply(square_sum_td, square_sum_query)\r\n",
    "    return numerator/(denom)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add new documents and check for plagarism"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "queries = [\r\n",
    "    'Feedback: feedback is typically used by the system to modify the query and improve prediction',\r\n",
    "    'information retrieval: ranking in information retrieval algorithms depends on user query'\r\n",
    "]\r\n",
    "\r\n",
    "for query in queries:\r\n",
    "    title, body = query.strip().split(\":\")\r\n",
    "    body = body.lstrip().lower()\r\n",
    "    header_freq = fit_count_frequency(list(title_set),[title.strip().lower()],True).ravel()\r\n",
    "    diff = np.sum(np.abs(title_array - header_freq), axis=1)\r\n",
    "    if not np.all(diff):\r\n",
    "        print(\"Duplicate Heading Detected: \", title)\r\n",
    "    else:\r\n",
    "        query_term,query_idf = get_tfidf(list(content_set), [body])\r\n",
    "        cosine_similarities = cosine_similarity_fn(weights_term_per_doc, query_idf).ravel()\r\n",
    "        if max(cosine_similarities) < 0.85:\r\n",
    "            rankedlist = sorted(range(len(cosine_similarities)), key=lambda i: cosine_similarities[i], reverse=True)[:10]\r\n",
    "            print(\"Top related documents for query\", query)\r\n",
    "            for index in rankedlist:\r\n",
    "                print(\"Document \"+ str(index+1),\"\\t\", cosine_similarities[index])\r\n",
    "                titles.append(title.strip().lower())\r\n",
    "                contents.append(body.strip().lower())\r\n",
    "                title_list = get_preprocessed_content(titles)\r\n",
    "                content_list = get_preprocessed_content(contents)\r\n",
    "                header_array = fit_count_frequency(title_list, titles, True)\r\n",
    "                weights_term_per_doc = fit_tfidf(content_list, contents)\r\n",
    "                print(\"=============================\")\r\n",
    "        else:\r\n",
    "            print(\"Plagirism detected with existing document\")\r\n",
    "            print(\"Document related closer to \", sorted(range(len(cosine_similarities)), key=lambda i: cosine_similarities[i], reverse=True)[0])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top related documents for query Feedback: feedback is typically used by the system to modify the query and improve prediction\n",
      "Document 2 \t 0.4311580502744009\n",
      "=============================\n",
      "Document 1 \t 0.38075433594185293\n",
      "=============================\n",
      "Document 3 \t 0.26065261909906917\n",
      "=============================\n",
      "Document 4 \t 0.0\n",
      "=============================\n",
      "Duplicate Heading Detected:  information retrieval\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Code for generating K shingles and binary jaccard similariy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def generate_kshingles(k,contents):\r\n",
    "    c_vectorizer = CountVectorizer(analyzer='word', ngram_range=(k,k))\r\n",
    "    return c_vectorizer, c_vectorizer.fit_transform(contents).toarray()\r\n",
    "\r\n",
    "def jaccard_binary(x,y):\r\n",
    "    intersection = np.logical_and(x, y)\r\n",
    "    union = np.logical_or(x, y)\r\n",
    "    similarity = intersection.sum() / float(union.sum())\r\n",
    "    return similarity\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating 3-shingles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "vectorizer, count_array = generate_kshingles(3 ,contents)\r\n",
    "for i in range(count_array.shape[0]):\r\n",
    "    for j in range(count_array.shape[0]):\r\n",
    "        if i != j and i < j:\r\n",
    "            jaccard_similarity = jaccard_binary(count_array[i], count_array[j])\r\n",
    "            print(\"Similarity between \", i+1 , \" and \", j+1 , \" is \", jaccard_similarity)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity between  1  and  2  is  0.0\n",
      "Similarity between  1  and  3  is  0.0\n",
      "Similarity between  1  and  4  is  0.0\n",
      "Similarity between  1  and  5  is  0.0\n",
      "Similarity between  1  and  6  is  0.0\n",
      "Similarity between  1  and  7  is  0.0\n",
      "Similarity between  1  and  8  is  0.0\n",
      "Similarity between  2  and  3  is  0.0\n",
      "Similarity between  2  and  4  is  0.07142857142857142\n",
      "Similarity between  2  and  5  is  0.0\n",
      "Similarity between  2  and  6  is  0.0\n",
      "Similarity between  2  and  7  is  0.0\n",
      "Similarity between  2  and  8  is  0.0\n",
      "Similarity between  3  and  4  is  0.0\n",
      "Similarity between  3  and  5  is  0.0\n",
      "Similarity between  3  and  6  is  0.0\n",
      "Similarity between  3  and  7  is  0.0\n",
      "Similarity between  3  and  8  is  0.0\n",
      "Similarity between  4  and  5  is  0.0\n",
      "Similarity between  4  and  6  is  0.0\n",
      "Similarity between  4  and  7  is  0.0\n",
      "Similarity between  4  and  8  is  0.0\n",
      "Similarity between  5  and  6  is  1.0\n",
      "Similarity between  5  and  7  is  1.0\n",
      "Similarity between  5  and  8  is  1.0\n",
      "Similarity between  6  and  7  is  1.0\n",
      "Similarity between  6  and  8  is  1.0\n",
      "Similarity between  7  and  8  is  1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Source Code Plagarism Detection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to read code from the specified directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def read_code(dir_name):\r\n",
    "    import os\r\n",
    "    data = []\r\n",
    "    for filename in os.listdir(dir_name):\r\n",
    "        with open(os.path.join(dir_name, filename), 'r') as f:\r\n",
    "            doc_content = \"\"\r\n",
    "            for line in f.readlines():\r\n",
    "                doc_content += line.strip() + \" \"\r\n",
    "            data.append(doc_content)\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read original code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "data = read_code('code')\r\n",
    "print(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['for i in range(0, 5): print(\"Hello Python\") ']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preproccess the python code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "processed_data = get_preprocessed_content(data)\r\n",
    "print(processed_data)\r\n",
    "\r\n",
    "term_freq_cs, weights_term_per_doc_cs = fit_tfidf(processed_data, data)\r\n",
    "print(weights_term_per_doc_cs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['5', 'range', '0', 'python', 'print', 'hello']\n",
      "[[0.  0.5 0.  0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read duplicate code for queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "queries = read_code('dup_code')\r\n",
    "print(queries)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['print(\"Hello Python\") print(\"Hello Python\") print(\"Hello Python\") print(\"Hello Python\") print(\"Hello Python\") ']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare docs with cosine similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "for i,query in enumerate(queries):\r\n",
    "    body = query.lower()\r\n",
    "    query_term,query_idf = get_tfidf(processed_data, [body])\r\n",
    "    cosine_similarities = cosine_similarity_fn(weights_term_per_doc_cs, query_idf).ravel()\r\n",
    "    print(\"Similarity of p{0} with Documents corpus\".format(i+1))\r\n",
    "    for cs_i,cs in enumerate(cosine_similarities):\r\n",
    "        print(\"Code-{0}\".format(cs_i),\" \",cs)\r\n",
    "    print(\"=======================\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity of p1 with Documents corpus\n",
      "Code-0   0.8660254037844386\n",
      "=======================\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}